{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36d930cf",
   "metadata": {
    "jupyter": {
     "source_hidden": false
    }
   },
   "source": [
    "# Introduction\n",
    "\n",
    "To understand the data present in `search_history.json`, we\n",
    "- Wrote `extract_schema` and `merge_schemas`: a mapping and reducer function that follows the mapReduce pattern to batch process \n",
    "  search_history.json entries in parallel to extract schema fields.\n",
    "- We check for any falsy values; that is, empty lists, empty dictionaries, empty strings, and whitespace only results.\n",
    "- We randomly sampled different entries to infer what the optional fields did; this was done as there was no documentation online for what\n",
    "  each optional field did.\n",
    "\n",
    "The search history data we are given in `search_history.json` is a Google Takeout export of search activity containing 55383 search entries\n",
    "accumulated over 7 years from June 2017 to June 2024 with no falsy values. The lists below describe the schema and the meaning of both mandatory and optional fields in the schema.\n",
    "\n",
    "**Mandatory Fields**\n",
    "\n",
    "- `header` (str): The Google product category. Only `\"Search\"` was observed across all entries.\n",
    "- `title` (str): Description of the activity. The following templates have been observed:\n",
    "\n",
    "  | Template                   | Meaning                                                    |\n",
    "  |----------------------------|------------------------------------------------------------|\n",
    "  | \"Searched for ...\"         | User ran a Google search                                   |\n",
    "  | \"Visited ...\"              | User visited a website from search results                 |\n",
    "  | \"Viewed ...\"               | User clicked on a Google Maps entry from search results    |\n",
    "  | \"1 notification\"           | User received a notification from Google Alerts            |\n",
    "  | \"Used Search\"              | Unknown - appears sparsely (170 times)                     |\n",
    "  | \"Ran internet speed test\"  | Unknown - appears only once                                |\n",
    "\n",
    "  Here, the \"...\" is either a search string or a URL.\n",
    "- `time` (str): ISO 8601 timestamp of when the activity occured. In the data, it ranged from `2017-06-08 16:42:55.223000+00:00` to\n",
    "  `2024-06-23 22:21:50.431000+00:00` for a total duration of 2572 days (7.0 years).\n",
    "- `products` (list): Google products involved. Only `[\"Search\"]` was observed across all entries.\n",
    "- `activityControls` (list): Which activity control settings captured this data. Only `[\"Web & App Activity\"]` was observed across all entries.\n",
    "\n",
    "**Optional Fields**\n",
    "\n",
    "- `titleUrl` (str, 55,021 / 99.3%): The URL associated with the activity. In the data, it only appeared for the `title` templates \"Searched\n",
    "  for ...\" and \"Visited ...\".\n",
    "- `details` (list, 2,794 / 5.0%): Additional details about the activity. In the data, when the field was present, only `[{'name': 'From Google Ads'}]` was observed, indicating that the search result the user interacted with was recommended by Google Ads.\n",
    "- `locationInfos` (list, 226 / 0.4%): Location data when search was made. This field only appears when you give Google permission to access \n",
    "    your location for the device you are performing the search for. The following location sources were observed:\n",
    "\n",
    "  | source                       | count |\n",
    "  |------------------------------|-------|\n",
    "  | \"From your places (Home)\"    | 175   |\n",
    "  | \"From your places (Work)\"    | 50    |\n",
    "  | \"Based on your past activity\"| 1     |\n",
    "\n",
    "  Here, \"From your places\" specifically refers to the \"Labelled\" location lists in Google Maps indicating your work and home locations. \n",
    "  In the data specifically, when the field was present, only the following values were observed:\n",
    "\n",
    "  ```json\n",
    "  // From your places (Home)\n",
    "  {\n",
    "    \"name\": \"At this general area\",\n",
    "    \"url\": \"https://www.google.com/maps/@?api=1&map_action=map&center=51.504495,-0.011733&zoom=12\",\n",
    "    \"source\": \"From your places (Home)\",\n",
    "    \"sourceUrl\": \"https://support.google.com/maps/answer/3184808\"\n",
    "  }\n",
    "\n",
    "  // From your places (Work)\n",
    "  {\n",
    "    \"name\": \"At this general area\",\n",
    "    \"url\": \"https://www.google.com/maps/@?api=1&map_action=map&center=51.525493,-0.082217&zoom=12\",\n",
    "    \"source\": \"From your places (Work)\",\n",
    "    \"sourceUrl\": \"https://support.google.com/maps/answer/3184808\"\n",
    "  }\n",
    "\n",
    "  // Based on your past activity\n",
    "  {\n",
    "    \"name\": \"At this general area\",\n",
    "    \"url\": \"https://www.google.com/maps/@?api=1&map_action=map&center=51.504467,-0.082155&zoom=12\",\n",
    "    \"source\": \"Based on your past activity\"\n",
    "  }\n",
    "  ```\n",
    "- `subtitles` (list, 166 / 0.3%): Additional context for the search. The only time it appears is for the `title` \"1 notification\" and\n",
    "  \"Ran internet speed test\". A breakdown of the subfields within `subtitles` are as follows:\n",
    "  - `subtitles[].name` (str): Subtitle text (e.g., topic names like \"Reuters\")\n",
    "  - `subtitles[].url` (str, 8): Optional URL within subtitle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f26b1b1",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": "import json\nfrom concurrent.futures import ThreadPoolExecutor\nfrom functools import reduce\n\n\ndef extract_schema(obj, prefix=\"\"):\n    \"\"\"Map: extract all key paths and types from a dict.\"\"\"\n    paths = {}\n    match obj:\n        case dict(d):\n            for k, v in d.items():\n                path = f\"{prefix}.{k}\" if prefix else k\n                paths[path] = {\"types\": {type(v).__name__}, \"count\": 1}\n                nested = extract_schema(v, path)\n                for p, info in nested.items():\n                    paths[p] = info\n        case list(items) if items:\n            paths[f\"{prefix}[]\"] = {\"types\": {\"list\"}, \"count\": 1}\n            nested = extract_schema(items[0], f\"{prefix}[]\")\n            for p, info in nested.items():\n                paths[p] = info\n        case list():\n            paths[f\"{prefix}[]\"] = {\"types\": {\"list\"}, \"count\": 1}\n    return paths\n\n\ndef merge_schemas(a, b):\n    \"\"\"Reduce: merge two schemas, combining types and counts.\"\"\"\n    result = dict(a)\n    for k, v in b.items():\n        if k in result:\n            result[k] = {\n                \"types\": result[k][\"types\"] | v[\"types\"],\n                \"count\": result[k][\"count\"] + v[\"count\"],\n            }\n        else:\n            result[k] = v\n    return result\n\n\ndef find_empty_values(obj, prefix=\"\"):\n    \"\"\"Map: find all empty/null values using structural pattern matching.\"\"\"\n    issues = {}\n    match obj:\n        case None:\n            issues[f\"{prefix} (null)\"] = 1\n        case \"\":\n            issues[f\"{prefix} (empty string)\"] = 1\n        case str(s) if s.isspace():\n            issues[f\"{prefix} (whitespace only)\"] = 1\n        case list() if not obj:\n            issues[f\"{prefix} (empty list)\"] = 1\n        case dict() if not obj:\n            issues[f\"{prefix} (empty dict)\"] = 1\n        case dict(d):\n            for k, v in d.items():\n                path = f\"{prefix}.{k}\" if prefix else k\n                nested = find_empty_values(v, path)\n                for p, count in nested.items():\n                    issues[p] = issues.get(p, 0) + count\n        case list(items):\n            for item in items:\n                nested = find_empty_values(item, f\"{prefix}[]\")\n                for p, count in nested.items():\n                    issues[p] = issues.get(p, 0) + count\n        case _:\n            pass  # Non-empty primitive value\n    return issues\n\n\ndef merge_issues(a, b):\n    \"\"\"Reduce: merge two issue dicts, summing counts.\"\"\"\n    result = dict(a)\n    for k, v in b.items():\n        result[k] = result.get(k, 0) + v\n    return result\n\n\nif __name__ == \"__main__\":\n    with open(\"../src/search_history.json\", \"r\") as f:\n        data = json.load(f)\n\n    total = len(data)\n    print(f\"Processing {total} entries...\\n\")\n\n    with ThreadPoolExecutor() as executor:\n        schemas = list(executor.map(extract_schema, data))\n        empty_values = list(executor.map(find_empty_values, data))\n\n    # Schema analysis\n    full_schema = reduce(merge_schemas, schemas, {})\n\n    mandatory = []\n    optional = []\n\n    for path in sorted(full_schema.keys()):\n        info = full_schema[path]\n        types = \", \".join(sorted(info[\"types\"]))\n        count = info[\"count\"]\n\n        if count == total:\n            mandatory.append((path, types))\n        else:\n            optional.append((path, types, count))\n\n    print(\"=== MANDATORY FIELDS (100%) ===\")\n    for path, types in mandatory:\n        print(f\"  {path}: {types}\")\n\n    print(f\"\\n=== OPTIONAL FIELDS ===\")\n    for path, types, count in optional:\n        print(f\"  {path}: {types} ({count}/{total})\")\n\n    # Empty values analysis\n    combined = reduce(merge_issues, empty_values, {})\n\n    if combined:\n        print(\"\\n=== EMPTY/NULL VALUES FOUND ===\")\n        for path, count in sorted(combined.items(), key=lambda x: -x[1]):\n            print(f\"  {path}: {count} occurrences\")\n    else:\n        print(\"\\nNo empty or null values found.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tuv3o6py2kn",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": "import json\nimport random\n\nOPTIONAL_FIELDS = [\"titleUrl\", \"details\", \"locationInfos\", \"subtitles\"]\nSAMPLE_SIZE = 5\n\n\ndef sample_by_field(data, field, n=SAMPLE_SIZE):\n    \"\"\"Get random sample of entries containing the specified field.\"\"\"\n    with_field = [e for e in data if field in e]\n    without_field = [e for e in data if field not in e]\n    return {\n        \"with\": random.sample(with_field, min(n, len(with_field))),\n        \"without\": random.sample(without_field, min(n, len(without_field))),\n    }\n\n\ndef format_samples(samples, field):\n    \"\"\"Format samples for a field as a single string.\"\"\"\n    with_entries = \"\\n\".join(\n        f\"\"\"\nSample {i}:\n  title: {entry.get('title', 'N/A')}\n  {field}: {entry.get(field)}\"\"\"\n        for i, entry in enumerate(samples[\"with\"], 1)\n    )\n\n    without_entries = \"\\n\".join(\n        f\"\"\"\nSample {i}:\n  title: {entry.get('title', 'N/A')}\"\"\"\n        for i, entry in enumerate(samples[\"without\"], 1)\n    )\n\n    return f\"\"\"\n{'='*60}\nFIELD: {field}\n{'='*60}\n\n--- Entries WITH {field} ---\n{with_entries}\n\n--- Entries WITHOUT {field} ---\n{without_entries}\n\"\"\"\n\n\nif __name__ == \"__main__\":\n    with open(\"../src/search_history.json\", \"r\") as f:\n        data = json.load(f)\n\n    for field in OPTIONAL_FIELDS:\n        samples = sample_by_field(data, field)\n        print(format_samples(samples, field))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nevjb9lnvu",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": "import json\nfrom collections import Counter\nfrom datetime import datetime\n\nCATEGORICAL_FIELDS = [\"header\", \"products\", \"activityControls\"]\n\n\ndef summarise_categorical(data, field):\n    \"\"\"Count occurrences of each unique value for a categorical field.\"\"\"\n    values = []\n    for entry in data:\n        val = entry.get(field)\n        if isinstance(val, list):\n            values.extend(val)\n        else:\n            values.append(val)\n    return Counter(values)\n\n\ndef summarise_time(data):\n    \"\"\"Find min and max timestamps.\"\"\"\n    times = [datetime.fromisoformat(e[\"time\"].replace(\"Z\", \"+00:00\")) for e in data]\n    return min(times), max(times)\n\n\nif __name__ == \"__main__\":\n    with open(\"../src/search_history.json\", \"r\") as f:\n        data = json.load(f)\n\n    for field in CATEGORICAL_FIELDS:\n        counts = summarise_categorical(data, field)\n        print(f\"\"\"\n{'='*60}\nFIELD: {field} ({len(counts)} unique values)\n{'='*60}\n{chr(10).join(f'  {val}: {count}' for val, count in counts.most_common())}\n\"\"\")\n\n    min_time, max_time = summarise_time(data)\n    duration = max_time - min_time\n    print(f\"\"\"\n{'='*60}\nFIELD: time\n{'='*60}\n  Min: {min_time}\n  Max: {max_time}\n  Duration: {duration.days} days ({duration.days / 365:.1f} years)\n\"\"\")"
  },
  {
   "cell_type": "markdown",
   "id": "9y794tqr1vk",
   "metadata": {},
   "source": "# Storing search history in a simplicial complex.\n\nThe 55,383 search entries spanning 7 years are semi-structured raw data. To process that into structured knowledge that captures user characteristics such as\n- *what* the user was interested in and *how* those interests relate.\n- a coherent narrative of the user's life and their tastes, values, etc.\nwe store it in a simplicial complex via a simplex tree. We have attached a paper justifying the use of the simplicial complex and the high level overview of the how retrieval and query pipelines are implemented for such a structure. This Python notebook primarily focuses on what a practical implementation of those pipelines could look like.\n\n## The Extraction Pipeline\n\nThe extraction pipeline (`src/pipeline.py`) processes each search entry through four stages:\n1. The `title` field encodes both activity type and content. We use pattern matching to parse and extract it:\n\n  | Title Pattern | Activity Type | Content |\n  |---------------|---------------|---------|\n  | \"Searched for jodhpur hotels\" | `searched` | \"jodhpur hotels\" |\n  | \"Visited https://example.com\" | `visited` | \"https://example.com\" |\n  | \"Viewed Umaid Bhawan Palace\" | `viewed` | \"Umaid Bhawan Palace\" |\n  | \"1 notification\" | `notification` | (extracted from subtitles) |\n\n2. For each content string, we extract entities and relationships. We call \n  the DedalusLabs SDK with a structured prompt:\n\n  ```\n  Given this search activity: \"jodhpur hotels\"\n  Extract:\n  - Entities mentioned (proper nouns, places, concepts)\n  - Relationships between entities (subject, predicate, object)\n  ```\n\n  The LLM returns structured output:\n  - **Entities**: `[\"Jodhpur\", \"hotels\"]`\n  - **Relationships**: `[(\"hotels\", \"located_in\", \"Jodhpur\")]`\n  and each entity becomes a vertex with an embedding, while each relationship ecomes a directed edge. Here, we use the `openai/text-embedding-3-small` due to time and cost considerations.\n\n3. We build simplexes for timestamp and location using witness complexes by \n  grouping entries by temporal and location proximity. Entries within a 30-minute window are assumed to be part of the same search session. For example, suppose in a 30 minute window, the user has the following search queries:\n  - \"jodhpur hotels\", which resolves to the list of entities `[\"jodhpur\", \"hotels\"]`.\n  - \"umaid bhawan palace\", which resolves to the list of entities \n    `[\"umaid bhawan\", \"palace\"]`.\n  - \"rajasthan travel\", which resolves to the list of entities `[\"rajasthan\", \"travel\"]`\n  Then, the resulting temporal simplex constructed is `[\"jodhpur\", \"hotels\", \"umaid bhawan\", \"palace\", \"rajasthan\", \"trave\"]`.\n\n4. The resulting simplex is inserted into the simplex tree as a branch in the trie\n  structure. As SQLite (the database used in this current implementation) does not support hash indexes, B-tree indexes are used in place. Despite so, the operations defined in the paper are still relatively efficient, with the following new time complexities:\n- **Search**: O(j log n) to check if a simplex exists\n- **Insert**: O(j log n) to add a new simplex\n- **Coface lookup**: O(k T log n) to find all simplices containing given vertices\n\nDue to API rate limits and time constraints, we've processed **1,410 entries** (2.5%) of the full dataset. The pipeline supports checkpointing—it can be resumed at any time to continue processing with the command below.\n\n```bash\ncd src\npython pipeline.py search_history.json --delay 0.1\n```\n\nThis partial dataset is sufficient to demonstrate the retrieval pipeline, though gap detection becomes more meaningful with denser coverage."
  },
  {
   "cell_type": "markdown",
   "id": "xtq9z31rjbo",
   "metadata": {},
   "source": [
    "## The Retrieval Pipeline\n",
    "\n",
    "The retrieval pipeline (`src/retrieval.py`) processes a query through four stages:\n",
    "\n",
    "1. The query is embedded using the same model (`openai/text-embedding-3-small`) and compared against all stored vertex embeddings via cosine similarity. Vertices above a threshold form the **query set Q**.\n",
    "\n",
    "  | Query | Matched Vertices (Q) |\n",
    "  |-------|---------------------|\n",
    "  | \"What hotel was I looking at in Jodhpur?\" | `[\"Jodhpur\", \"hotel\", \"Umaid Bhawan Palace\"]` |\n",
    "\n",
    "2. For each vertex in Q, we find all simplices containing it via coface lookup. If `\"Jodhpur\"` appears in a 3-simplex `{Jodhpur, Umaid Bhawan, heritage hotel}`, we retrieve that entire co-occurrence pattern. The union of all cofaces forms the **query-induced subcomplex K_Q**. This surfaces related entities the user may have forgotten—context the graph remembers.\n",
    "\n",
    "3. For each coface, we enumerate its **theoretical faces** (all 2^n - 1 subsets) and check which are missing from the database. Missing faces represent **knowledge gaps**. For example, if we have:\n",
    "  - 2-simplex `{Jodhpur, Umaid Bhawan, hotel}` (from one session)\n",
    "  - 1-simplex `{Jodhpur, Mehrangarh Fort}` (from another session)\n",
    "  - But no simplex containing `{Umaid Bhawan, Mehrangarh Fort}`\n",
    "\n",
    "  Then `{Umaid Bhawan, Mehrangarh Fort}` is a gap—the user searched for both Jodhpur attractions but never together. The agent can infer a connection but should flag lower confidence.\n",
    "\n",
    "4. The pipeline assembles context for the LLM: matched vertices with similarity scores, coface contents with their temporal/location context, explicit edges, and knowledge gaps. The `format_context()` method renders this as:\n",
    "\n",
    "  ```\n",
    "  === Matched Entities ===\n",
    "    - Jodhpur (similarity: 0.89)\n",
    "    - Umaid Bhawan Palace (similarity: 0.72)\n",
    "\n",
    "  === Co-occurrence Patterns (Simplices) ===\n",
    "    - [from 2023-05-01T10:30:00 to 2023-05-01T10:45:00] {Jodhpur, Umaid Bhawan Palace, heritage hotel}\n",
    "    - [at home] {Jodhpur, travel planning, flight booking}\n",
    "\n",
    "  === Known Relationships ===\n",
    "    - (Umaid Bhawan Palace) --[located_in]--> (Jodhpur)\n",
    "\n",
    "  === Knowledge Gaps (Unconfirmed Relationships) ===\n",
    "    - {Umaid Bhawan Palace, Mehrangarh Fort} - never directly observed together\n",
    "  ```\n",
    "\n",
    "  The temporal and location context helps the agent construct a narrative: the user researched Jodhpur hotels during a specific session, and also searched for Jodhpur-related topics from home on other occasions. In practice, one might give explicit instructions to an LLM agent to ingest this information and produce a chain of thought reasoning about the user's narrative before handing over the results to another LLM agent that is responsible for the actual query output. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bsrj8c0d25c",
   "metadata": {},
   "source": "## Usage\n\n**Extraction:**\n```bash\ncd src\npython pipeline.py search_history.json --delay 0.1 --window 30\n```\n\n**Querying:**\n```bash\ncd src\npython retrieval.py \"What hotel was I looking at in Jodhpur?\" --top-k 10 --threshold 0.3\n```"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}