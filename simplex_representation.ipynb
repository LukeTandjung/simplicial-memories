{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": 0,
   "id": "36d930cf",
   "metadata": {
    "jupyter": {
     "source_hidden": false
    }
   },
   "source": [
    "# Introduction\n",
    "\n",
    "To understand the data present in `search_history.json`, we\n",
    "- Wrote `extract_schema` and `merge_schemas`: a mapping and reducer function that follows the mapReduce pattern to batch process \n",
    "  search_history.json entries in parallel to extract schema fields.\n",
    "- We check for any falsy values; that is, empty lists, empty dictionaries, empty strings, and whitespace only results.\n",
    "- We randomly sampled different entries to infer what the optional fields did; this was done as there was no documentation online for what\n",
    "  each optional field did.\n",
    "\n",
    "The search history data we are given in `search_history.json` is a Google Takeout export of search activity containing 55383 search entries\n",
    "accumulated over 7 years from June 2017 to June 2024 with no falsy values. The lists below describe the schema and the meaning of both mandatory and optional fields in the schema.\n",
    "\n",
    "**Mandatory Fields**\n",
    "\n",
    "- `header` (str): The Google product category. Only \"Search\" was observed across all entries.\n",
    "- `title` (str): Description of the activity. The following templates have been observed:\n",
    "\n",
    "  | Template                   | Meaning                                                    |\n",
    "  |----------------------------|------------------------------------------------------------|\n",
    "  | \"Searched for ...\"         | User ran a Google search                                   |\n",
    "  | \"Visited ...\"              | User visited a website from search results                 |\n",
    "  | \"Viewed ...\"               | User clicked on a Google Maps entry from search results    |\n",
    "  | \"1 notification\"           | User received a notification from Google Alerts            |\n",
    "  | \"Used Search\"              | Unknown - appears sparsely (170 times)                     |\n",
    "  | \"Ran internet speed test\"  | Unknown - appears only once                                |\n",
    "\n",
    "  Here, the \"...\" is either a search string or a URL.\n",
    "- `time` (str): ISO 8601 timestamp of when the activity occured. In the data, it ranged from 2017-06-08 16:42:55.223000+00:00 to\n",
    "  2024-06-23 22:21:50.431000+00:00 for a total duration of 2572 days (7.0 years).\n",
    "- `products` (list): Google products involved. Only [\"Search\"] was observed across all entries.\n",
    "- `activityControls` (list): Which activity control settings captured this data. Only [\"Web & App Activity\"] was observed across all entries.\n",
    "\n",
    "**Optional Fields**\n",
    "\n",
    "- `titleUrl` (str, 55,021 / 99.3%): The URL associated with the activity. In the data, it only appeared for the `title` templates \"Searched\n",
    "  for ...\" and \"Visited ...\".\n",
    "- `details` (list, 2,794 / 5.0%): Additional details about the activity. In the data, when the field was present, \n",
    "  - `details[].name` (str): Name/description within details\n",
    "- `locationInfos` (list, 226 / 0.4%): Location data when search was made\n",
    "  - `locationInfos[].name` (str): Location description (e.g., \"At this general area\")\n",
    "  - `locationInfos[].url` (str): Google Maps link to the location\n",
    "  - `locationInfos[].source` (str): Source of location (e.g., \"From your places (Home)\")\n",
    "  - `locationInfos[].sourceUrl` (str, 225 / 0.4%): Help URL explaining the location source\n",
    "- `subtitles` (list, 166 / 0.3%): Additional context like notification topics\n",
    "  - `subtitles[].name` (str): Subtitle text (e.g., topic names like \"Reuters\")\n",
    "  - `subtitles[].url` (str, 8): Optional URL within subtitle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f26b1b1",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 55383 entries...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MANDATORY FIELDS (100%) ===\n",
      "  activityControls: list\n",
      "  activityControls[]: list\n",
      "  header: str\n",
      "  products: list\n",
      "  products[]: list\n",
      "  time: str\n",
      "  title: str\n",
      "\n",
      "=== OPTIONAL FIELDS ===\n",
      "  details: list (2794/55383)\n",
      "  details[]: list (2794/55383)\n",
      "  details[].name: str (2794/55383)\n",
      "  locationInfos: list (226/55383)\n",
      "  locationInfos[]: list (226/55383)\n",
      "  locationInfos[].name: str (226/55383)\n",
      "  locationInfos[].source: str (226/55383)\n",
      "  locationInfos[].sourceUrl: str (225/55383)\n",
      "  locationInfos[].url: str (226/55383)\n",
      "  subtitles: list (166/55383)\n",
      "  subtitles[]: list (166/55383)\n",
      "  subtitles[].name: str (166/55383)\n",
      "  subtitles[].url: str (8/55383)\n",
      "  titleUrl: str (55021/55383)\n",
      "\n",
      "No empty or null values found.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from functools import reduce\n",
    "\n",
    "\n",
    "def extract_schema(obj, prefix=\"\"):\n",
    "    \"\"\"Map: extract all key paths and types from a dict.\"\"\"\n",
    "    paths = {}\n",
    "    match obj:\n",
    "        case dict(d):\n",
    "            for k, v in d.items():\n",
    "                path = f\"{prefix}.{k}\" if prefix else k\n",
    "                paths[path] = {\"types\": {type(v).__name__}, \"count\": 1}\n",
    "                nested = extract_schema(v, path)\n",
    "                for p, info in nested.items():\n",
    "                    paths[p] = info\n",
    "        case list(items) if items:\n",
    "            paths[f\"{prefix}[]\"] = {\"types\": {\"list\"}, \"count\": 1}\n",
    "            nested = extract_schema(items[0], f\"{prefix}[]\")\n",
    "            for p, info in nested.items():\n",
    "                paths[p] = info\n",
    "        case list():\n",
    "            paths[f\"{prefix}[]\"] = {\"types\": {\"list\"}, \"count\": 1}\n",
    "    return paths\n",
    "\n",
    "\n",
    "def merge_schemas(a, b):\n",
    "    \"\"\"Reduce: merge two schemas, combining types and counts.\"\"\"\n",
    "    result = dict(a)\n",
    "    for k, v in b.items():\n",
    "        if k in result:\n",
    "            result[k] = {\n",
    "                \"types\": result[k][\"types\"] | v[\"types\"],\n",
    "                \"count\": result[k][\"count\"] + v[\"count\"],\n",
    "            }\n",
    "        else:\n",
    "            result[k] = v\n",
    "    return result\n",
    "\n",
    "\n",
    "def find_empty_values(obj, prefix=\"\"):\n",
    "    \"\"\"Map: find all empty/null values using structural pattern matching.\"\"\"\n",
    "    issues = {}\n",
    "    match obj:\n",
    "        case None:\n",
    "            issues[f\"{prefix} (null)\"] = 1\n",
    "        case \"\":\n",
    "            issues[f\"{prefix} (empty string)\"] = 1\n",
    "        case str(s) if s.isspace():\n",
    "            issues[f\"{prefix} (whitespace only)\"] = 1\n",
    "        case list() if not obj:\n",
    "            issues[f\"{prefix} (empty list)\"] = 1\n",
    "        case dict() if not obj:\n",
    "            issues[f\"{prefix} (empty dict)\"] = 1\n",
    "        case dict(d):\n",
    "            for k, v in d.items():\n",
    "                path = f\"{prefix}.{k}\" if prefix else k\n",
    "                nested = find_empty_values(v, path)\n",
    "                for p, count in nested.items():\n",
    "                    issues[p] = issues.get(p, 0) + count\n",
    "        case list(items):\n",
    "            for item in items:\n",
    "                nested = find_empty_values(item, f\"{prefix}[]\")\n",
    "                for p, count in nested.items():\n",
    "                    issues[p] = issues.get(p, 0) + count\n",
    "        case _:\n",
    "            pass  # Non-empty primitive value\n",
    "    return issues\n",
    "\n",
    "\n",
    "def merge_issues(a, b):\n",
    "    \"\"\"Reduce: merge two issue dicts, summing counts.\"\"\"\n",
    "    result = dict(a)\n",
    "    for k, v in b.items():\n",
    "        result[k] = result.get(k, 0) + v\n",
    "    return result\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    with open(\"./search_history.json\", \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    total = len(data)\n",
    "    print(f\"Processing {total} entries...\\n\")\n",
    "\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        schemas = list(executor.map(extract_schema, data))\n",
    "        empty_values = list(executor.map(find_empty_values, data))\n",
    "\n",
    "    # Schema analysis\n",
    "    full_schema = reduce(merge_schemas, schemas, {})\n",
    "\n",
    "    mandatory = []\n",
    "    optional = []\n",
    "\n",
    "    for path in sorted(full_schema.keys()):\n",
    "        info = full_schema[path]\n",
    "        types = \", \".join(sorted(info[\"types\"]))\n",
    "        count = info[\"count\"]\n",
    "\n",
    "        if count == total:\n",
    "            mandatory.append((path, types))\n",
    "        else:\n",
    "            optional.append((path, types, count))\n",
    "\n",
    "    print(\"=== MANDATORY FIELDS (100%) ===\")\n",
    "    for path, types in mandatory:\n",
    "        print(f\"  {path}: {types}\")\n",
    "\n",
    "    print(f\"\\n=== OPTIONAL FIELDS ===\")\n",
    "    for path, types, count in optional:\n",
    "        print(f\"  {path}: {types} ({count}/{total})\")\n",
    "\n",
    "    # Empty values analysis\n",
    "    combined = reduce(merge_issues, empty_values, {})\n",
    "\n",
    "    if combined:\n",
    "        print(\"\\n=== EMPTY/NULL VALUES FOUND ===\")\n",
    "        for path, count in sorted(combined.items(), key=lambda x: -x[1]):\n",
    "            print(f\"  {path}: {count} occurrences\")\n",
    "    else:\n",
    "        print(\"\\nNo empty or null values found.\")"
   ]
  },
  {
   "cell_type": "code",
   "id": "tuv3o6py2kn",
   "source": "import json\nimport random\n\nOPTIONAL_FIELDS = [\"titleUrl\", \"details\", \"locationInfos\", \"subtitles\"]\nSAMPLE_SIZE = 5\n\n\ndef sample_by_field(data, field, n=SAMPLE_SIZE):\n    \"\"\"Get random sample of entries containing the specified field.\"\"\"\n    with_field = [e for e in data if field in e]\n    without_field = [e for e in data if field not in e]\n    return {\n        \"with\": random.sample(with_field, min(n, len(with_field))),\n        \"without\": random.sample(without_field, min(n, len(without_field))),\n    }\n\n\ndef format_samples(samples, field):\n    \"\"\"Format samples for a field as a single string.\"\"\"\n    with_entries = \"\\n\".join(\n        f\"\"\"\nSample {i}:\n  title: {entry.get('title', 'N/A')}\n  {field}: {entry.get(field)}\"\"\"\n        for i, entry in enumerate(samples[\"with\"], 1)\n    )\n\n    without_entries = \"\\n\".join(\n        f\"\"\"\nSample {i}:\n  title: {entry.get('title', 'N/A')}\"\"\"\n        for i, entry in enumerate(samples[\"without\"], 1)\n    )\n\n    return f\"\"\"\n{'='*60}\nFIELD: {field}\n{'='*60}\n\n--- Entries WITH {field} ---\n{with_entries}\n\n--- Entries WITHOUT {field} ---\n{without_entries}\n\"\"\"\n\n\nif __name__ == \"__main__\":\n    with open(\"./search_history.json\", \"r\") as f:\n        data = json.load(f)\n\n    for field in OPTIONAL_FIELDS:\n        samples = sample_by_field(data, field)\n        print(format_samples(samples, field))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "nevjb9lnvu",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "FIELD: header (1 unique values)\n",
      "============================================================\n",
      "  Search: 55383\n",
      "\n",
      "\n",
      "============================================================\n",
      "FIELD: products (1 unique values)\n",
      "============================================================\n",
      "  Search: 55383\n",
      "\n",
      "\n",
      "============================================================\n",
      "FIELD: activityControls (1 unique values)\n",
      "============================================================\n",
      "  Web & App Activity: 55383\n",
      "\n",
      "\n",
      "============================================================\n",
      "FIELD: time\n",
      "============================================================\n",
      "  Min: 2017-06-08 16:42:55.223000+00:00\n",
      "  Max: 2024-06-23 22:21:50.431000+00:00\n",
      "  Duration: 2572 days (7.0 years)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "\n",
    "CATEGORICAL_FIELDS = [\"header\", \"products\", \"activityControls\"]\n",
    "\n",
    "\n",
    "def summarise_categorical(data, field):\n",
    "    \"\"\"Count occurrences of each unique value for a categorical field.\"\"\"\n",
    "    values = []\n",
    "    for entry in data:\n",
    "        val = entry.get(field)\n",
    "        if isinstance(val, list):\n",
    "            values.extend(val)\n",
    "        else:\n",
    "            values.append(val)\n",
    "    return Counter(values)\n",
    "\n",
    "\n",
    "def summarise_time(data):\n",
    "    \"\"\"Find min and max timestamps.\"\"\"\n",
    "    times = [datetime.fromisoformat(e[\"time\"].replace(\"Z\", \"+00:00\")) for e in data]\n",
    "    return min(times), max(times)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    with open(\"./search_history.json\", \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    for field in CATEGORICAL_FIELDS:\n",
    "        counts = summarise_categorical(data, field)\n",
    "        print(f\"\"\"\n",
    "{'='*60}\n",
    "FIELD: {field} ({len(counts)} unique values)\n",
    "{'='*60}\n",
    "{chr(10).join(f'  {val}: {count}' for val, count in counts.most_common())}\n",
    "\"\"\")\n",
    "\n",
    "    min_time, max_time = summarise_time(data)\n",
    "    duration = max_time - min_time\n",
    "    print(f\"\"\"\n",
    "{'='*60}\n",
    "FIELD: time\n",
    "{'='*60}\n",
    "  Min: {min_time}\n",
    "  Max: {max_time}\n",
    "  Duration: {duration.days} days ({duration.days / 365:.1f} years)\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}